%!TEX TS-options = --shell-escape
%!TEX TS-program = pdflatex
\documentclass[%
   10pt,              % Schriftgroesse
                 % wird an andere Pakete weitergereicht
   a4paper,           % Seitengroesse
   DIV10,             % Textbereichsgroesse (siehe Koma Skript Dokumentation !)
]{scrartcl}%     Klassen: scrartcl, scrreprt, scrbook, article
% -------------------------------------------------------------------------

\usepackage[utf8]{inputenc} % Font Encoding, benoetigt fuer Umlaute
%\usepackage[ngerman]{babel}   % Spracheinstellung


\usepackage{ulem}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage{multirow}
\usepackage{listings}
\usepackage{ifthen}
\usepackage{todonotes}
\usepackage{mathtools}
\usetikzlibrary{automata,arrows}


% Definition des Headers
\usepackage{geometry}
\geometry{a4paper, top=3cm, left=3cm, right=3cm, bottom=3cm, headsep=0mm, footskip=0mm}
\renewcommand{\baselinestretch}{1.3}\normalsize
\renewcommand{\labelenumi}{\alph{enumi})}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\def\header#1#2#3#4#5#6#7{\pagestyle{empty}
\noindent
\begin{minipage}[t]{0.6\textwidth}
\begin{flushleft}
\textbf{#4}\\% Fach
#6\\% Semester
#2  % Tutor 
\end{flushleft}
\end{minipage}
\begin{minipage}[t]{0.4\textwidth}
\begin{flushright}
\points{#7}% Punktetabelle
\vspace*{0.2cm}
#5%  Names
\end{flushright}
\end{minipage}

\begin{center}
{\Large\textbf{ Assignment #1}} % Blatt

{(Abgabe am #3)} % Abgabedatum
\end{center}
}

\newenvironment{vartab}[1]
{
    \begin{tabular}{ |c@{} *{#1}{c|} } %\hline
}{
    \end{tabular}
}

\newcommand{\myformat}[1]{& #1}

\newcommand{\entry}[1]{
  \edef\result{\csvloop[\myformat]{#1}}
  \result \\ \hline
}

\newcommand{\numbers}[1]{
  \newcounter{ctra}
\setcounter{ctra}{1}
\whiledo {\value{ctra} < #1}%
{%
  \myformat{\thectra}
  \stepcounter{ctra}%
}
\myformat{\thectra}
}
\newcommand{\emptyLine}[1]{
  \newcounter{ctra1}
\setcounter{ctra}{1}
\whiledo {\value{ctra1} < #1}%
{%
  \myformat{\hspace*{0.5cm}}
  \stepcounter{ctra1}%
}
}


\newcommand{\points}[1]{
\newcounter{colmns}
\setcounter{colmns}{#1}
\stepcounter{colmns}
  \begin{vartab}{\thecolmns}
    \numbers{#1} & $\sum$\\\hline
    \emptyLine{\thecolmns}\\
  \end{vartab}

}

\begin{document}
%\header{Blatt}{Tutor}{Abgabedatum}{Vorlesung}{Bearbeiter}{Semester}{Anzahl Aufgaben}
\header{1}{}{24. April 2018}{ML: Algo \& Theory}{Florence Lopez (3878792), florence.lopez@student.uni-tuebingen.de \\ Jennifer Them (3837649), jennifer.them@student.uni-tuebingen.de}{SS 18}{4}

\section*{Exercise 1}
\begin{itemize}
	\item[a.)] In the following we will abbreviate small with $s$, medium with $me$, large with $l$, female with $f$ and male with $m$:\begin{itemize}
		\item $P(Y = s) = P(X=m,Y=s) + P(X=f,Y=s) = 0.1 + 0.3 = 0.4 = 40\%$
		\item $P(Y = me) = P(X=m,Y=me) + P(X=f,Y=me) = 0.15 + 0.1 = 0.25 = 25\%$
		\item $P(Y = l) = P(X=m,Y=l) + P(X=f,Y=l) = 0.25 + 0.1 = 0.35 = 35\%$
		\item $P(X = m) = P(X=m,Y=s) + P(X=m,Y=me) + P(X=m,Y=l) = 0.1 + 0.15 + 0.25 = 0.5 = 50\%$
		\item $P(X = f) = P(X=f,Y=s) + P(X=f,Y=me) + P(X=f,Y=l) = 0.3 + 0.1 + 0.1 = 0.5 = 50\%$
	\end{itemize}
	\item[b.)] Proof: Choose a random $\epsilon > 0$. According to the Tchebycheff Inequality, we set $k = \epsilon$ and $X = \overline{X}$. $\overline{X}$ needs to be a random variable within the same distribution as the empirical mean $\overline{x}$. Then the following applies:\newline
	
	\noindent $P(|\overline{X} - E(\overline{X})| \geq \epsilon) \leq \frac{Var(\overline{X})}{\epsilon^2}$.\newline
	
	\noindent Because $\overline{X} = \frac{\sum_{i=1}^{n} X_i}{n}, E(X_i) = E(X), Var(X_i) = Var(X)$ and since all the $X_i$ are iid, we get:\newline
	
	\noindent $E(\overline{X}) = E(\frac{\sum_{i=1}^{n} X_i}{n}) = \frac{\sum_{i=1}^n E(X_i)}{n} = \frac{\sum_{i=1}^n E(X)}{n} = \frac{n \cdot E(X)}{n} = E(X)$.\newline
	\noindent $Var(\overline{X}) = Var(\frac{\sum_{i=1}^{n} X_i}{n^2}) = \frac{\sum_{i=1}^n Var(X_i)}{n^2} = \frac{\sum_{i=1}^n Var(X)}{n^2} = \frac{n \cdot Var(X)}{n^2} = \frac{Var(X)}{n}$.\newline
	
	\noindent Therefore we get the following: $P(|\overline{X} - E(X)| \geq \epsilon) \leq \frac{Var(X)}{n \cdot \epsilon^2}$. The $lim_{n \rightarrow \infty} \frac{Var(X)}{n \cdot \epsilon^2} = 0$. This leads to: $P(|\overline{X} - E(X)| > \epsilon) \leq P(|\overline{X} - E(X)| \geq \epsilon)$, which proves the weak law of large numbers. 
\end{itemize}

\section*{Exercise 2}
\begin{itemize}
	\item[a.)] $P(X=me|Y=f) = \frac{P(X=me~\cap~Y=f)}{P(Y=f)} = \frac{0.1}{0.5} = 0.02$
	\item[b.)] Two random variables $X,Y$ are indepent if and only if $P(X \cap Y) = P(X) \cdot P(Y)$ or if $P(X|Y) = P(X)$. This means that the random variable $Y$ has nor influence on the probability of $X$ and vice-versa.
	\item[c.)] In the following we will abbreviate a positive test with $+$, a negative test with $-$, cancer with $c$ and no cancer with $\not c$:
	\begin{itemize}
		\item $P(A=+|B=c) = 0.95$\newline
		\noindent $P(A=-|B=\not c) = 0.95$\newline
		\noindent $P(B=c) = 0.01$
		\item $P(B=c|A=+) = \frac{P(A=+|B=c) \cdot P(B=c)}{P(A=+)}$
		\item $P(A=+)$ can be calculated with marginalisation:\newline
		\noindent $P(A=+) = P(A=+|B=c) \cdot P(B=c) + P(A=+|B=\not c) \cdot P(B=\not c)$, with $P(A=+|B=\not c)$ being 0.05. This results in $P(A=+) = 0.95 \cdot 0.01 + 0.05 \cdot 0.99 = 0.059 = 5.9\%$
		\item This results in: $P(B=c|A=+) = \frac{0.95 \cdot 0.01}{0.059} = 0.16 = 16\%$
	\end{itemize}
\end{itemize}

\section*{Exercise 3}
\begin{itemize}
	\item[a.)] $A \cdot x = \begin{pmatrix}
	1 & 1 & 2\\
	1 & 2 & 1\\
	5 & 7 & 8
	\end{pmatrix} \cdot \begin{bmatrix}
	x_1\\
	x_2\\
	x_3
	\end{bmatrix} = \begin{bmatrix}
	a_1 & a_2 & a_3
	\end{bmatrix} \cdot \begin{bmatrix}
	x_1\\
	x_2\\
	x_3
	\end{bmatrix} = \begin{bmatrix}
	a_1
	\end{bmatrix} \cdot x_1 + \begin{bmatrix}
	a_2
	\end{bmatrix} \cdot x_2 + \begin{bmatrix}
	a_3
	\end{bmatrix} \cdot x_3 = \begin{bmatrix}
	1\\
	1\\
	5
	\end{bmatrix} \cdot x_1 + \begin{bmatrix}
	1\\
	2\\
	7
	\end{bmatrix} \cdot x_2 + \begin{bmatrix}
	2\\
	1\\
	8
	\end{bmatrix} \cdot x_3 = \begin{bmatrix}
	x_1 + x_2 + x_3\\
	x_1 + 2x_2 + x_3\\
	5x_1 + 7x_2 + 8x_3
	\end{bmatrix}$
	
	\item[b.)] Columns: If the columns of $A$ should form a Basis of $\mathbb{R}^3$, one needs to test if the column-vectors are linearly independent and if they are able to produce every single possible vector of $\mathbb{R}^3$. This is done in the following:\newline
	\noindent $\begin{bmatrix}
	x\\
	y\\
	z
	\end{bmatrix} = \lambda_1 \cdot \begin{bmatrix}
	1\\
	1\\
	5
	\end{bmatrix} + \lambda_2 \cdot \begin{bmatrix}
	1\\
	2\\
	7
	\end{bmatrix} + \lambda_3 \cdot \begin{bmatrix}
	2\\
	1\\
	8
	\end{bmatrix}$.\newline\noindent 
	
	This results in the following LGS:\newline\noindent
	\begin{align*}
	\text{I)} && \lambda_1 + \lambda_2 + 2\lambda_3 &= x &&\\
	\text{II)} && \lambda_1 + 2\lambda_2 + \lambda_3&= y &&\\
	\text{III)} && 5\lambda_1 + 7\lambda_2 + 8\lambda_3&= z &&\\[-1.7ex]
	\cline{1-6}
	\text{I)} && x - \lambda_2 - 2\lambda_3 &=\lambda_1 &&\\
	\text{$\lambda_1$ in II)} && (x - \lambda_2 - 2\lambda_3) + 2\lambda_2 + \lambda_3 &= y\\
	\text{$\Leftrightarrow$} && x+ \lambda_2 - \lambda_3 &= y\\
	\text{$\Leftrightarrow$} && y + \lambda_3 - x &= \lambda_2&&\\[-1.7ex]
	\cline{1-6}
	\text{$\lambda_1, \lambda_2$ in III)} &&  5(x- \lambda_2 -2\lambda_3) + 7(y + \lambda_3 - x) + 8 \lambda_3 &= z\\
	\text{$\Leftrightarrow$} && 3x + 2y + 10\lambda_3 &=z\\
	\text{$\Leftrightarrow$} && (z-3x -2y)/10&= \lambda_3\\
	\end{align*}
	
	$\lambda_1, \lambda_2, \lambda_3$ are therefore all three $\in \mathbb{R}^3$ and explicit, which means that all three column vectors are linearly indepent and also able to build all possible vectors in $\mathbb{R}^3$, which means that there are a basis of $\mathbb{R}^3$.\newline
	
	\noindent Rows: 
	If the rows of $A$ should form a Basis of $\mathbb{R}^3$, one needs to test if the row-vectors are linearly independent and if they are able to produce every single possible vector of $\mathbb{R}^3$. This is done in the following:\newline
	\noindent $\begin{bmatrix}
	x\\
	y\\
	z
	\end{bmatrix} = \lambda_1 \cdot \begin{bmatrix}
	1\\
	1\\
	2
	\end{bmatrix} + \lambda_2 \cdot \begin{bmatrix}
	1\\
	2\\
	1
	\end{bmatrix} + \lambda_3 \cdot \begin{bmatrix}
	5\\
	7\\
	8
	\end{bmatrix}$.\newline\noindent 
	
	This results in the following LGS:\newline\noindent
	\begin{align*}
		\text{I)} && \lambda_1 + \lambda_2 + 5\lambda_3 &= x &&\\
		\text{II)} && \lambda_1 + 2\lambda_2 + 7\lambda_3&= y &&\\
		\text{III)} && 2\lambda_1 + 1\lambda_2 + 8\lambda_3&= z &&\\[-1.7ex]
		\cline{1-6}
		\text{I)} && x - \lambda_2 - 5\lambda_3 &=\lambda_1 &&\\
		\text{$\lambda_1$ in II)} && (x - \lambda_2 - 5\lambda_3) + 2\lambda_2 + 7\lambda_3 &= y\\
		\text{$\Leftrightarrow$} && x+ \lambda_2 + 2\lambda_3 &= y\\
		\text{$\Leftrightarrow$} && y - x -2\lambda_3 &= \lambda_2&&\\[-1.7ex]
		\cline{1-6}
		\text{$\lambda_1, \lambda_2$ in III)} &&  2(x- \lambda_2 -5\lambda_3) + (y - x - 2\lambda_3) + 8 \lambda_3 &= z\\
		\text{$\Leftrightarrow$} && x - 2\lambda_2 - 4\lambda_3 + y &=z\\
		\text{$\Leftrightarrow$} && 3x - y&= z\\
	\end{align*}
	
	Since $\lambda_3$ can be chosen freely the row vectors of $A$ are not linearly indepent from each other, which means that they are not building a Basis of the $\mathbb{R}^3$.\newline
	
	\item[c.)] Considering $b = (2, 3, 12)$:\newline\noindent
	$A \cdot x = b \Leftrightarrow \begin{pmatrix}
	1 & 1 & 2\\
	1 & 2 & 1\\
	5 & 7 & 8
	\end{pmatrix} \cdot \begin{bmatrix}
	x\\
	y\\
	z
	\end{bmatrix} = \begin{bmatrix}
	2\\
	3\\
	12
	\end{bmatrix}$. This results in the following LGS:\newline\noindent
	\begin{align*}
	\text{I)} && x_1 + x_2 + 2x_3 &= 2 &&\\
	\text{II)} && x_1 + 2x_2 + x_3 &= 3 &&\\
	\text{III)} && 5x_1 + 7x_2 + 8x_3 &= 12 &&\\[-1.7ex]
	\cline{1-6}
	\text{I)} && 2 - x_2 - 2x_3 &=x_1 &&\\
	\text{$x_1$ in II)} && 2 - x_2 - 2x_3 + 2x_2 + x_3 &= 3\\
	\text{$\Leftrightarrow$} && 2 + x_2 - x_3  &= 3\\
	\text{$\Leftrightarrow$} && 1 + x_3 &= x_2&&\\[-1.7ex]
	\cline{1-6}
	\text{$x_1, x_2$ in III)} &&  5(2 - x_2 - 2x_3) + 7(1 + x_3) + 8x_3 &= 12\\
	\text{$\Leftrightarrow$} && 10 - 5x_2 - 10x_3 + 7 + 7x_3 + 8x_3 &=12\\
	\text{$\Leftrightarrow$} && 5 + 7&= 12~\surd\\
	\end{align*}
	
	This means that $x_3$ can be chosen freely. We chose $x_3 = 1$, which leads to $x_1 = -2$ and $x_2 = 2$. This leads to $x = \begin{bmatrix}
	-2\\
	2\\
	1
	\end{bmatrix}$
	
	\item[d.)] \begin{itemize}
		\item Column rank: The column rank is 3, since $A$ has 3 linearly indepent column vectors.
		\item Row rank: The row rank is 2, since there are only 2 linearly indepent row vectors, namely $r_1 = \begin{bmatrix}
			1\\
			1\\
			2
		\end{bmatrix}$ and $r_2 = \begin{bmatrix}
		1\\
		2\\
		1
		\end{bmatrix}$.
		\item Rank of $A$: The rank of $A$ is 3, since the maximum rank of column and row rank is 3. 
	\end{itemize}
	
\end{itemize}


\section*{Exercise 4}
\begin{itemize}
	\item[a.)] \begin{itemize}
		\item $A = \begin{pmatrix}
		1 & 0\\
		0 & 1
		\end{pmatrix}$ has the independent eigenvectors $u_1 = (1, 0)$ and $u_2 = (0,1)$ and the eigenvalue $\lambda = 1$.
		\item $B = \begin{pmatrix}
		1 & 2\\
		2 & 1
		\end{pmatrix}$ has the independent eigenvectors $u_1 = (1, 1)$ with eigenvalue $\lambda_1 = 3$ and $u_2 = (-1, 1)$ with eigenvalue $\lambda_2 = -1$.
		\item $C = \begin{pmatrix}
		1 & 1\\
		0 & 1
		\end{pmatrix}$ has the independent eigenvector $u_1 = (1, 0)$ with eigenvalue $\lambda = 1$. In the case of this matrix, one can observe the following:\begin{itemize}
			\item $x-axis > 0, y-axis > 0$: $C \cdot x$ is larger than $x$. 
			\item $x-axis > 0, y-axis < 0$: $C \cdot x$ is smaller than $x$. 
			\item $x-axis < 0, y-axis < 0$: $C \cdot x$ is larger than $x$. 
			\item $x-axis < 0, y-axis > 0$: $C \cdot x$ is smaller than $x$. 
		\end{itemize}
	\end{itemize}
	\item[b.)] In the following we will always use this equation: $A \cdot u_i = \lambda_i \cdot I \cdot u_i$. \begin{enumerate} 
		\item[(1)] $(A + \alpha \cdot I) \cdot u_i = A\cdot u_i + \alpha \cdot I \cdot u_i = \lambda_i \cdot I \cdot u_i + \alpha \cdot I \cdot u_i = \lambda_i \cdot u_i + \alpha \cdot u_i = (\lambda_i + \alpha) \cdot u_i$. This results in the eigenvalue $\lambda_i + \alpha$.
		\item [(2)] Since $A$ is a symmetric matrix, the following applies: $A^T = A$. This results in $A^T A = AA$. Therefore we have: $AA \cdot u_i = A \cdot (\lambda_i \cdot u_i) = \lambda_i \cdot (A \cdot u_i) = \lambda_i \cdot (\lambda_i \cdot u_i) = \lambda_i^2 \cdot u_i$. This results in the eigenvalue $\lambda_i^2$.
		\item[(3)] Analogous to (2) $A^T = A$, so this results in the eigenvalue $\lambda_i^2$. 
		\item[(4)] In general the following applies: $A \cdot u_i = = \lambda_i \cdot u_i$. Now, we multiply $A^{-1}$ to this equation and get: $A^{-1} \cdot A \cdot u_i = \lambda_i \cdot A^{-1} \cdot u_i \Leftrightarrow I \cdot u_i = \lambda_i \cdot A^{-1} \cdot u_i \Leftrightarrow \lambda_i^{-1} \cdot u_i = A^{-1}\cdot u_i$. Therefore the eigenvalues would be $\lambda_i^{-1}$. 
	\end{enumerate}
	\item[c.)] A matrix $S$ can be decomposed in the following way: $S = U \cdot D \cdot V^T$. Given $SS^T$ and $S^TS$, the following to equations apply:\newline
	
	\noindent $S^TS = (UDV^T)^T \cdot (UDV^T) = VD^TU^T \cdot UDV^T = VD^T \cdot DV^T = V (D^TD) V$\newline
	\noindent $SS^T = (UDV^T) \cdot (UDV^T)^T = UDV^T \cdot VD^TU^T = UD \cdot D^TU^T = U (D^TD) U$
\end{itemize}


\end{document}
%!TEX TS-options = --shell-escape
%!TEX TS-program = pdflatex
\documentclass[%
   10pt,              % Schriftgroesse
                 % wird an andere Pakete weitergereicht
   a4paper,           % Seitengroesse
   DIV10,             % Textbereichsgroesse (siehe Koma Skript Dokumentation !)
]{scrartcl}%     Klassen: scrartcl, scrreprt, scrbook, article
% -------------------------------------------------------------------------

\usepackage[utf8]{inputenc} % Font Encoding, benoetigt fuer Umlaute
%\usepackage[ngerman]{babel}   % Spracheinstellung


\usepackage{ulem}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage{multirow}
\usepackage{listings}
\usepackage{ifthen}
\usepackage{todonotes}
\usepackage{mathtools}
\usetikzlibrary{automata,arrows}


% Definition des Headers
\usepackage{geometry}
\geometry{a4paper, top=3cm, left=3cm, right=3cm, bottom=3cm, headsep=0mm, footskip=0mm}
\renewcommand{\baselinestretch}{1.3}\normalsize
\renewcommand{\labelenumi}{\alph{enumi})}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\def\header#1#2#3#4#5#6#7{\pagestyle{empty}
\noindent
\begin{minipage}[t]{0.6\textwidth}
\begin{flushleft}
\textbf{#4}\\% Fach
#6\\% Semester
#2  % Tutor 
\end{flushleft}
\end{minipage}
\begin{minipage}[t]{0.4\textwidth}
\begin{flushright}
\points{#7}% Punktetabelle
\vspace*{0.2cm}
#5%  Names
\end{flushright}
\end{minipage}

\begin{center}
{\Large\textbf{ Assignment #1}} % Blatt

{(Abgabe am #3)} % Abgabedatum
\end{center}
}

\newenvironment{vartab}[1]
{
    \begin{tabular}{ |c@{} *{#1}{c|} } %\hline
}{
    \end{tabular}
}

\newcommand{\myformat}[1]{& #1}

\newcommand{\entry}[1]{
  \edef\result{\csvloop[\myformat]{#1}}
  \result \\ \hline
}

\newcommand{\numbers}[1]{
  \newcounter{ctra}
\setcounter{ctra}{1}
\whiledo {\value{ctra} < #1}%
{%
  \myformat{\thectra}
  \stepcounter{ctra}%
}
\myformat{\thectra}
}
\newcommand{\emptyLine}[1]{
  \newcounter{ctra1}
\setcounter{ctra}{1}
\whiledo {\value{ctra1} < #1}%
{%
  \myformat{\hspace*{0.5cm}}
  \stepcounter{ctra1}%
}
}


\newcommand{\points}[1]{
\newcounter{colmns}
\setcounter{colmns}{#1}
\stepcounter{colmns}
  \begin{vartab}{\thecolmns}
    \numbers{#1} & $\sum$\\\hline
    \emptyLine{\thecolmns}\\
  \end{vartab}

}

\begin{document}
%\header{Blatt}{Tutor}{Abgabedatum}{Vorlesung}{Bearbeiter}{Semester}{Anzahl Aufgaben}
\header{3}{}{08. Mai 2018}{ML: Algo \& Theory}{Florence Lopez (3878792), florence.lopez@student.uni-tuebingen.de \\ Jennifer Them (3837649), jennifer.them@student.uni-tuebingen.de}{SS 18}{4}

\section*{Exercise 1}

\begin{itemize}
	\item[a.)] Compute the derivative $\frac{\delta f}{\delta X}$, where $X = \begin{bmatrix}
	x_1\\
	x_2\\
	x_3
	\end{bmatrix}$:\newline
	\noindent $f(X) = a^T X  = \begin{bmatrix}
	2 & -1 & 5
	\end{bmatrix} \cdot \begin{bmatrix}
	x_1\\
	x_2\\
	x_3
	\end{bmatrix} = 2x_1 - x_2 + 5x_3$.\newline
	
	\noindent $\frac{\delta f}{\delta X} f(X) = \begin{bmatrix}
	\frac{\delta f}{\delta x_1} & \frac{\delta f}{\delta x_2} & \frac{\delta f}{\delta x_3}
	\end{bmatrix}$ with $\frac{\delta f}{\delta x_1} = 2$, $\frac{\delta f}{\delta x_2} = -1$, $\frac{\delta f}{\delta x_3} = 5$.\newline 
	This leads to $\frac{\delta f}{\delta X} = \begin{bmatrix}
	2 & -1 & 5 
	\end{bmatrix} = a^T$.
	
	\item[b.)] Compute the derivative $\frac{\delta f}{\delta X}$, where $X = \begin{bmatrix}
	x_1\\
	x_2
	\end{bmatrix}$:\newline
	\noindent $f(X) = X^T AX  = \begin{bmatrix}
	x_1& x_2
	\end{bmatrix} \cdot \begin{pmatrix}
	1 & 2\\
	5 & 3
	\end{pmatrix} \cdot \begin{bmatrix}
	x_1\\
	x_2
	\end{bmatrix}  = \begin{bmatrix}
	x_1& x_2
	\end{bmatrix} \cdot \begin{pmatrix}
	x_1 + 2x_2\\
	5x_1 + 3x_2
	\end{pmatrix} = x_1^2 + 3x_2^2 + 7x_1 x_2$.\newline
	
	\noindent $\frac{\delta f}{\delta X} f(X) = \begin{bmatrix}
	\frac{\delta f}{\delta x_1} & \frac{\delta f}{\delta x_2}
	\end{bmatrix} = \begin{bmatrix}
	2x_1 + 7x_2 & 7x_1 + 6x_2
	\end{bmatrix} = \begin{bmatrix}
	x_1 & x_2
	\end{bmatrix} \cdot \begin{pmatrix}
	2 & 7\\
	7 & 6
	\end{pmatrix} = x^T \cdot (A + A^T)$.
\end{itemize}

\section*{Exercise 2}

\begin{itemize}
	\item[a.)] We have to show that $X^tX$ is positive semidefinite and that rank($X^tX$) = rank($X$). 
	\begin{enumerate}
		\item[1.] A matrix $X$ is positive semidefinite, if for all $v \in \mathbb{R}^n$ the following holds: $v^t X v \geq 0$. In this case we will need $v^t X^tX v \geq 0$. Since $v^t X^t = X v$, we will have to prove that $X v X v \geq 0$. $X v X v \geq 0 \Leftrightarrow \langle Xv, Xv\rangle \geq 0$. The multiplication of $v^t X^t$ gives us an $1xn$ vector and the multiplication of $X v$ gives us an $nx1$ vector. Therefore we will have a single value at the end of the whole multiplication. The scalar product of $\langle Xv, Xv\rangle$ equals $|| Xv||^2$, which will always be $\geq 0$, since all values of $Xv$ multiplied with $Xv$ will be positive, even if the single entries of $Xv$ are negative. Therefore $X^tX$ is a positive semidefinite matrix.
		\item[2.] rank($X^t X$) = rank($X$). This relation is called the rank-nullity theorem. 
		\end{enumerate}
	\item[b.)] We have to show that $X(\omega^* - \omega_0) = 0$ with $\omega_0 = (X^tX)^+X^tY$. To show this, we have to show that $\omega^* = \omega_0$, because this would be the solution to $X(\omega^* - \omega_0) = 0$. Since $\omega^*$ is the optimal solution for the regression problem, we have to show that $\min_\omega|| Y - X\omega||^2 = (X^tX)^+X^tY$. To show this, we will compute the derivative of $|| Y - X\omega||^2$ in the following steps: 
	\begin{enumerate}
		\item[1.] $|| Y - X\omega||^2$ can be represented in matrix notation: $(Y - X\omega)^t \cdot (Y - X\omega) \Leftrightarrow (Y^t - X^t\omega^t) \cdot (Y - X\omega) \Leftrightarrow Y^t Y - \omega^t X^t Y - Y^t X \omega + \omega^t X^t X \omega \Leftrightarrow Y^t Y - 2\omega^t X^t Y + \omega^t X^t X \omega$. The last step could take place, because of the following equation: $\omega^t X^t Y = (\omega^t X^t Y)^t = Y^t X \omega$.
		\item[2.] To find the optimal solution, we have to derivate the above equation: $\frac{\delta}{\delta\omega} Y^t Y - 2\omega^t X^t Y + \omega^t X^t X \omega$. We will split up this equation in 2 parts and derive them.
		\item[3.] First part: $\frac{\delta}{\delta \omega} \omega^t X^t Y = \frac{\delta}{\delta \omega} Y^t X \omega = Y^t X = X^t Y$.
		\item[4.] Second part: $\frac{\delta}{\delta \omega} \omega^t X^t X \omega = \frac{\delta}{\delta \omega} \omega^t X^t + \frac{\delta}{\delta \omega} X\omega = X^t X\omega + X X^t \omega^t = 2 X^t X \omega$, because $X X^t \omega^t = X^t X \omega$.
		\item[5.] If we put all parts together, we get the overall derivation and set it to zero:\newline
		\noindent $\frac{\delta}{\delta \omega} Y^t Y - 2\omega^t X^t Y + \omega^t X^t X \omega = 0 \Leftrightarrow -2 X^t Y + 2 X^t X \omega = 0\Leftrightarrow 2 X^t X \omega = 2 X^t Y \Leftrightarrow X^t X \omega = X^t Y \Leftrightarrow \omega = (X^t X)^+ X^t Y$, which is the solution for $\omega_0$.
	\end{enumerate}
	\noindent Therefore one can say that $\omega^* = \omega_0 = (X^t X)^+ X^t Y$, which leads to $X(\omega^* - \omega_0) = 0$.
	\item[c.)] -
\end{itemize}

\section*{Exercise 3}

\begin{itemize}
	\item[a.)] Inserting all 4 $X, Y$-pairs we get the following LGS with 4 equations and $\omega = \begin{bmatrix}
	\omega_1\\
	\omega_2\\
	\omega_3
	\end{bmatrix}$:\newline\noindent 
	\begin{align*}
	\text{I)} && \omega_1 + \omega_2 &= 3 &&\\
	\text{II)} && \omega_1 - \omega_2 + 2\omega_3&= 1 &&\\
	\text{III)} && 2\omega_1 + 3\omega_2 - \omega_3&= 7 &&\\
	\text{IV)} && -\omega_1 + 2\omega_2 - 3\omega_3&= 0 &&\\[-1.7ex]
	\cline{1-6}
	\text{I)} && 3 - \omega_1&=\omega_2 &&\\
	\text{$\omega_2$ in II)} && \omega_1 - (3 -\omega_1) + 2\omega_3&= 1\\
	\text{$\Leftrightarrow$} && 2\omega_1 + 2\omega_3&= 4\\
	\text{$\Leftrightarrow$} && 2 - \omega_1&= \omega_3&&\\[-1.7ex]
	\cline{1-6}
	\text{$\omega_2, \omega_3$ in III)} &&  2\omega_1 + 3(3 - \omega_1) - (2 - \omega_1)&= 7\\
	\text{$\Leftrightarrow$} && 2 \omega_1 + 9 - 3\omega_1 - 2 \omega_1&=7\\
	\text{$\Leftrightarrow$} && 2 \omega_1 - 3\omega_1 + \omega_1&= 0\\
	\text{$\Leftrightarrow$} && 0&= 0\\
	\end{align*} 
	\noindent This means that $\omega_1$ can be chosen freely. We chose $\omega_1 = 1$, which leads to $\omega_2 = 2$ and $\omega_3 = 1$ and therefore we have $\omega = \begin{bmatrix}
	1\\
	2\\
	1
	\end{bmatrix}$ as first optimal solution. We can obtain other optimal solutions, since we can chose $\omega_1$ freely. We also chose the $\omega_1 = 2$, which led to $\omega_2 = 1$ and $\omega_3 = 0$. The last optimal solution we obtained was with $\omega_1 = 3$, which led to $\omega_2 = 0$ and $\omega_3 = -1$. So we obtain 3 different optimal solutions:\newline
	\noindent $\omega^1 = \begin{bmatrix}
	1\\
	2\\
	1
	\end{bmatrix}, \omega^2 = \begin{bmatrix}
	2\\
	1\\
	0
	\end{bmatrix}$ and $\omega^3 = \begin{bmatrix}
		3\\
		0\\
		-1
	\end{bmatrix}$.
	\item[b.)] Predicting $\hat{Y}$ with $X_5 = (0, 2, -2)$:
	\begin{itemize}
		\item $\omega^1$: $\hat{Y} = X_5 \cdot \omega^1 = (0, 2, -2) \cdot \begin{bmatrix}
		1\\
		2\\
		1
		\end{bmatrix} = 2$.
		\item $\omega^2$: $\hat{Y} = X_5 \cdot \omega^2 = (0, 2, -2) \cdot \begin{bmatrix}
		2\\
		1\\
		0
		\end{bmatrix} = 4$.
		\item $\omega^2$: $\hat{Y} = X_5 \cdot \omega^3 = (0, 2, -2) \cdot \begin{bmatrix}
		3\\
		0\\
		-1
		\end{bmatrix} = 2$
	\end{itemize}

	\noindent Predicting $\hat{Y}$ with $X_6 = (1, 0, 0)$:
	\begin{itemize}
		\item $\omega^1$: $\hat{Y} = X_6 \cdot \omega^1 = (1, 0, 0) \cdot \begin{bmatrix}
		1\\
		2\\
		1
		\end{bmatrix} = 0$.
		\item $\omega^2$: $\hat{Y} = X_6 \cdot \omega^2 = (1, 0, 0) \cdot \begin{bmatrix}
		2\\
		1\\
		0
		\end{bmatrix} = 2$.
		\item $\omega^2$: $\hat{Y} =X_6 \cdot \omega^3 = (1, 0, 0) \cdot \begin{bmatrix}
		3\\
		0\\
		-1
		\end{bmatrix} = 3$
	\end{itemize}

	\noindent The predictions do not match, because we found the optimal solutions based on the freely chosen $\omega_1$, therefore the solutions are not the same for the 3 different optimal solutions found in (a).
	\item[c.)] Inserting all 4 $X, Y$-pairs we get the following LGS with 4 equations and $\omega = \begin{bmatrix}
	\omega_1\\
	\omega_2\\
	\omega_3
	\end{bmatrix}$:\newline\noindent
	\begin{align*}
	\text{I)} && 2\omega_1^2 + 2\omega_2^2 + \omega_3^2 &= 3 &&\\
	\text{II)} && 2\omega_1^2 + 3\omega_3^2&= 1 &&\\
	\text{III)} && 3\omega_1^2 + 4\omega_2^2&= 7 &&\\
	\text{IV)} && 3\omega_2^2 - 2\omega_3^2&= 0 &&\\[-1.7ex]
	\cline{1-6}
	\text{II)} && 2\omega_1^2 + 3\omega_3^2&=1&&\\
	\text{$\Leftrightarrow$} && \frac{1 - 2\omega_1^2}{3}&= \omega_3^2\\
	\text{$\omega_3^2$ in IV)} && 3\omega_2^2 - 2 \cdot (\frac{1 - 2\omega_1^2}{3})&= 0\\
	\text{$\Leftrightarrow$} && \frac{2}{3} + \frac{4}{3} \cdot \omega_1^2&= 3\omega_2^2\\
	\text{$\Leftrightarrow$} && \frac{2}{9} + \frac{4}{9} \cdot \omega_1^2&= \omega_2^2&&\\[-1.7ex]
	\cline{1-6}
	\text{$\omega_2^2, \omega_3^2$ in I)} &&  2\omega_1^2 + 2 \cdot (\frac{2}{9} + \frac{4}{9} \cdot \omega_1^2) + \frac{1 - 2\omega_1^2}{3}&= 3\\
	\text{$\Leftrightarrow$} && \frac{20}{9} \omega_1^2&=\frac{20}{9}\\
	\text{$\Leftrightarrow$} && \omega_1^2&= 1\\
	\text{$\Leftrightarrow$} && \omega_1&= 1\\
	\end{align*} 
	
	\noindent This leads to $\omega_2 = \sqrt{\frac{1}{3}} \approx 0.58$ and $\omega_3 = \sqrt{\frac{-1}{3}} = \frac{i}{\sqrt{3}}$, which leads to $\omega = \begin{bmatrix}
	1\\
	0.58\\
	\frac{i}{\sqrt{3}}
	\end{bmatrix}$ as an optimal solution for the ridge regression problem.
\end{itemize}

\section*{Exercise 4}
\begin{itemize}
	\item[a.)] see code in jupyter notebook.
	\item[b.)] see code in jupyter notebook.
	\item[c.)] see code in jupyter notebook. The MSE is the highest for $\lambda = 10$. The plots can be seen in the jupyter notebook. 
	\item[d.)] see code in jupyter notebook. As one can see in the plots the linear regression with $\lambda = 0$ gives us the same prediction as the ridge regression with $\lambda = 0.001$.
	\item[e.)] see code in jupyter notebook. As one can see the MSE is approximately the same for all different $\lambda$ and even for the linear regression. The best $\lambda$ could be $\lambda = 0.001$, since it produces a marginally smaller error than the other ones, but it seems like they all lead to the same result in the end.
	\item[f.)] - 
\end{itemize}


\end{document}
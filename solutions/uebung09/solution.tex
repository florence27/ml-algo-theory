%!TEX TS-options = --shell-escape
%!TEX TS-program = pdflatex
\documentclass[%
   10pt,              % Schriftgroesse
                 % wird an andere Pakete weitergereicht
   a4paper,           % Seitengroesse
   DIV10,             % Textbereichsgroesse (siehe Koma Skript Dokumentation !)
]{scrartcl}%     Klassen: scrartcl, scrreprt, scrbook, article
% -------------------------------------------------------------------------

\usepackage[utf8]{inputenc} % Font Encoding, benoetigt fuer Umlaute
%\usepackage[ngerman]{babel}   % Spracheinstellung


\usepackage{ulem}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage{multirow}
\usepackage{listings}
\usepackage{ifthen}
\usepackage{todonotes}
\usepackage{mathtools}
\usetikzlibrary{automata,arrows}


% Definition des Headers
\usepackage{geometry}
\geometry{a4paper, top=3cm, left=3cm, right=3cm, bottom=3cm, headsep=0mm, footskip=0mm}
\renewcommand{\baselinestretch}{1.3}\normalsize
\renewcommand{\labelenumi}{\alph{enumi})}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\def\header#1#2#3#4#5#6#7{\pagestyle{empty}
\noindent
\begin{minipage}[t]{0.6\textwidth}
\begin{flushleft}
\textbf{#4}\\% Fach
#6\\% Semester
#2  % Tutor 
\end{flushleft}
\end{minipage}
\begin{minipage}[t]{0.4\textwidth}
\begin{flushright}
\points{#7}% Punktetabelle
\vspace*{0.2cm}
#5%  Names
\end{flushright}
\end{minipage}

\begin{center}
{\Large\textbf{ Assignment #1}} % Blatt

{(Abgabe am #3)} % Abgabedatum
\end{center}
}

\newenvironment{vartab}[1]
{
    \begin{tabular}{ |c@{} *{#1}{c|} } %\hline
}{
    \end{tabular}
}

\newcommand{\myformat}[1]{& #1}

\newcommand{\entry}[1]{
  \edef\result{\csvloop[\myformat]{#1}}
  \result \\ \hline
}

\newcommand{\numbers}[1]{
  \newcounter{ctra}
\setcounter{ctra}{1}
\whiledo {\value{ctra} < #1}%
{%
  \myformat{\thectra}
  \stepcounter{ctra}%
}
\myformat{\thectra}
}
\newcommand{\emptyLine}[1]{
  \newcounter{ctra1}
\setcounter{ctra}{1}
\whiledo {\value{ctra1} < #1}%
{%
  \myformat{\hspace*{0.5cm}}
  \stepcounter{ctra1}%
}
}


\newcommand{\points}[1]{
\newcounter{colmns}
\setcounter{colmns}{#1}
\stepcounter{colmns}
  \begin{vartab}{\thecolmns}
    \numbers{#1} & $\sum$\\\hline
    \emptyLine{\thecolmns}\\
  \end{vartab}

}

\begin{document}
%\header{Blatt}{Tutor}{Abgabedatum}{Vorlesung}{Bearbeiter}{Semester}{Anzahl Aufgaben}
\header{8}{}{26. Juni 2018}{ML: Algo \& Theory}{Florence Lopez (3878792), florence.lopez@student.uni-tuebingen.de \\ Jennifer Them (3837649), jennifer.them@student.uni-tuebingen.de}{SS 18}{4}

\section*{Exercise 1}
\section*{Exercise 2}
\section*{Exercise 3}
\section*{Exercise 4}
\begin{itemize}
	\item[a.)] The following equation has to be shown: $\sum_{i \in C_k} \norm{X_i - m_k}^2 \leq \sum_{i \in C_k} \norm{X_i - X_j}^2, \forall j \in C_k$. In one of the latter exercises, we have already proven that a function with the form $w \rightarrow \norm{Y - Xw}^2$ is convex. By setting $X = Id$, the function of the form $w \rightarrow \norm{Y - w}$ is convex, too. Additionally, the sum of a two convex functions, is convex too. Since our function $\sum_{i \in C_k} \norm{X_i - m_k}^2$ is a sum of convex functions, it is convex, too. Therefore we can derive it and set it to 0:\\
	$\frac{\delta}{\delta x_l} \sum_{i \in C_k} \norm{X_i - x}^2 = \frac{\delta}{\delta x_l} \sum_{i \in C_k} \sum_{j = 0}^{d} (X_{ij} - x_j)^2 = \frac{\delta}{\delta x_l} \sum_{i \in C_k} \sum_{j = 0}^{d} (X_{ij}^2 - 2 X_{ij}x_j + x_j^2)^2 = \sum_{i \in C_k} 2x_l - 2X_{il} = 2 |C_k|x_l- 2 \sum_{i \in C_k} X_{il}$.\\
	Set this to 0, with: $\sum_{i \in C_k} 2x_l - 2X_{il} = 2 |C_k|x_l- 2 \sum_{i \in C_k} X_{il} = 0 \Leftrightarrow 2 |C_k|x_l = 2\sum_{i \in C_k} X_{il} X_{il} \Leftrightarrow x_l = \frac{\sum_{i \in C_k} X_{il}}{|C_k|} = \overline{X_{il}}$. This means that the cluster center in each dimension $l$ for all data points is the best solution. 
	\item[b.)] The k-means algorithm computes the new cluster centers for every step. Therefore, with the results from (a) we get an optimal solution $L_1$ with respect to the given partition of the data. In the next step the algorithm calculates a new solution $L_2$, which represents a new partition. Given these two solutions, we have two different cases that can occur: 
	\begin{enumerate}
		\item[1.] After getting two solutions $L_1$ and $L_2$ in two following time steps, the partitions remain the same. This means that our current cluster centers are optimal and the partition does not change anymore, it converges. Therefore the algorithm terminates here.
		\item[2.] After getting two solutions $L_1$ and $L_2$ in two following time steps, the partitions are not the same. This means that the second solution is more optimal than the first solution, leading to $L_2 \leq L_1$. This brings again two cases, which need to be differed: 
		\begin{enumerate}
			\item[1.] $L_2 < L_1$, meaning $L_2$ is truly smaller than $L_1$. In this case solutions from any later time step will always be smaller than $L_2$, meaning that the partition from $L_1$ will not occur again, since we already found a better partition with $L_2$. 
			\item[2.] $L_2 = L_1$, meaning the solutions of both time steps are equivalent, which leads to both partitions being equivalent to each other. In this case, it could be that the algorithm oscillates between two partitions, which would mean that it diverges. This problem could be solved by adjusting the algorithm in such a way that it always picks the old solution, if the new solution is equal to it. In this case the algorithm would converge and therefore terminate.  
		\end{enumerate}
	\end{enumerate}
\end{itemize}


\end{document}